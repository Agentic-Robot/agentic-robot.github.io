<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners -->
  <meta name="description" content="Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents">
  <meta property="og:title" content="Agentic Robot: Vision-Language-Action Models in Embodied Agents"/>
  <meta property="og:description" content="We propose Agentic Robot, a brain-inspired framework integrating Standardized Action Procedures (SAP) for reliable long-horizon robotic manipulation."/>
  <meta property="og:url" content="YOUR_PROJECT_URL_HERE"/>
  <meta property="og:image" content="static/image/agentic_robot_banner.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="Agentic Robot Framework">
  <meta name="twitter:description" content="Brain-inspired framework for embodied manipulation using SAP-driven coordination.">
  <meta name="twitter:image" content="static/images/agentic_robot_twitter_banner.png">
  <meta name="twitter:card" content="summary_large_image">

  <meta name="keywords" content="Robotics, Vision-Language-Action Models, Embodied AI, Robotic Manipulation, Standardized Action Procedures, SAP">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Agentic Robot: Vision-Language-Action Models in Embodied Agents</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


<!-- Authors -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a href="#" target="_blank">Zhejian Yang</a>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Yongchao Chen</a>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Xueyang Zhou</a>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Jiangyue Yan</a>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Dingjie Song</a>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Yinuo Liu</a>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Yuting Li</a>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Yu Zhang</a>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Pan Zhou</a>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Hechang Chen</a><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Lichao Sun</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Jilin University, Harvard University, Massachusetts Institute of Technology,<br>
              Huazhong University of Science and Technology, Southern University of Science and Technology,<br>
              Lehigh University, Shanghai Jiao Tong University
            </span>
            <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2505.23450" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End authors -->


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <img src="static/images/FLow.jpg" alt="Teaser Image" style="width:100%; height:auto;">
      <h2 class="subtitle has-text-centered">
        Overview of the Agentic Robot framework governed by Standardized Action Procedure (SAP). (1) A high-level task is decomposed into structured subgoals by an LRM-based planner, guided by a skill library. (2) A VLA policy as Executor executes each subgoal using natural language instructions and real-time visual input. (3) A VLM-based verifier periodically inspects a sliding window of third-person and wrist-mounted views to determine whether to continue, retry, or recover. This SAP-driven agentic loop enables robust, interpretable, and feedback-driven manipulation.
      </h2>
    </div>
    <div class="item has-text-centered">
      <!-- Your image here -->
      <img src="static/images/Recover.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Comparison between OpenVLA and Agentic Robot on the task ``Put the cream cheese in the bowl.'' \textbf{Top:} OpenVLA fails to grasp the object, causing the gripper to collide with the table and the task to fail. \textbf{Bottom:} Agentic Robot decomposes the task into subgoals and detects failure via visual verification. It issues a recovery action (\texttt{Lift the gripper}) and completes the task through retry.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Long-horizon robotic manipulation poses significant challenges for autonomous systems, requiring extended reasoning, precise execution, and robust error recovery across complex sequential tasks. Current approaches, whether based on static planning or end-to-end visuomotor policies, suffer from error accumulation and lack effective verification mechanisms during execution, limiting their reliability in real-world scenarios. We present Agentic Robot, a brain-inspired framework that addresses these limitations through Standardized Action Procedures (SAP)â€”a novel coordination protocol governing component interactions throughout manipulation tasks. Drawing inspiration from Standardized Operating Procedures (SOPs) in human organizations, SAP establishes structured workflows for planning, execution, and verification phases. Our architecture comprises three specialized components: (1) a large reasoning model that decomposes high-level instructions into semantically coherent subgoals, (2) a vision-language-action executor that generates continuous control commands from real-time visual inputs, and (3) a temporal verifier that enables autonomous progression and error recovery through introspective assessment. This SAP-driven closed-loop design supports dynamic self-verification without external supervision. On the LIBERO benchmark, Agentic Robot achieves state-of-the-art performance with an average success rate of 79.6%, outperforming SpatialVLA by 6.1% and OpenVLA by 7.4% on long-horizon tasks.  These results demonstrate that SAP-driven coordination between specialized components enhances both performance and interpretability in sequential manipulation, suggesting significant potential for reliable autonomous systems.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
<div class="hero-body">
  <div class="container">
    <h2 class="title is-3">Experimental Results</h2>
    <div id="results-carousel" class="carousel results-carousel">
      <div class="item has-text-centered">
        <!-- Your image here -->
        <img src="static/images/table1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          LIBERO benchmark results comparing success rates (SR) and standard error across four task suites, averaged over three random seeds with 500 evaluation trials. FT indicates fine-tuning on task-specific demonstrations. Bold values indicate best performance.
        </h2>
      </div>
      <div class="item has-text-centered">
        <!-- Your image here -->
        <img src="static/images/table2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Performance comparison between OpenVLA and Agentic Robot on LIBERO-Long tasks, showing success rates for individual subtasks and overall task success rate (SR).
       </h2>
      </div>
      <div class="item has-text-centered">
       <!-- Your image here -->
       <img src="static/images/fig3.jpg" alt="MY ALT TEXT"/>
       <h2 class="subtitle has-text-centered">
        Effect of verification frequency on performance across LIBERO task suites. Bars compare three settings where the VLM verifier is invoked every 10, 20, or 50 steps during execution.
      </h2>
      </div>
      <div class="item has-text-centered">
      <!-- Your image here -->
      <img src="static/images/table3.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Ablation study on LIBERO-Long. Each row represents success rate after removing a key component.
      </h2>
      </div>
    </div>
  </div>
</div>
</section>
<!-- End image carousel -->



<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Demonstration Videos</h2>
      <div id="video-item-carousel" class="carousel results-carousel">
        <div class="item item-video1 has-text-centered">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/47_1748436172.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2 has-text-centered">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/48_1748436172.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3 has-text-centered">
          <video poster="" id="video3" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/49_1748436172.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video4 has-text-centered">
          <video poster="" id="video4" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/50_1748436172.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->



<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{yang2025agenticrobotbraininspiredframework,
      title={Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents},
      author={Zhejian Yang and Yongchao Chen and Xueyang Zhou and Jiangyue Yan and Dingjie Song and Yinuo Liu and Yuting Li and Yu Zhang and Pan Zhou and Hechang Chen and Lichao Sun},
      year={2025},
      eprint={2505.23450},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2505.23450},
}</code></pre>
  </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
